# -DecisionTree
ID3算法构建决策树
（一）	实验内容
本实验主要利用7565组食物的22种性质和能否食用的数据作为训练集来构建一颗朴素的决策树。并利用另外851组数据来测试该决策树模型的准确程度。实验内容包括：分析与设计算法，利用特定数据结构存数据，构建决策树和算法不足分析这几部分。

（二）	算法说明
1.	决策树
决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。
决策树是一种依托决策而建立起来的一种树。在机器学习中，决策树是一种预测模型，代表的是一种对象属性与对象值之间的一种映射关系，每一个节点代表某个对象，树中的每一个分叉路径代表某个可能的属性值，而每一个叶子节点则对应从根节点到该叶子节点所经历的路径所表示的对象的值。决策树仅有单一输出，如果有多个输出，可以分别建立独立的决策树以处理不同的输出。

2.	信息增益
在信息增益中，重要性的衡量标准就是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。在认识信息增益之前，先来看看信息熵的定义：
熵这个概念最早起源于物理学，在物理学中是用来度量一个热力学系统的无序程度，而在信息学里面，熵是对不确定性的度量。在1948年，香农引入了信息熵，将其定义为离散随机事件出现的概率，一个系统越是有序，信息熵就越低，反之一个系统越是混乱，它的信息熵就越高。所以信息熵可以被认为是系统有序化程度的一个度量。假设一个随机变量的取值为： ，每种性质取到的概率为  ，则信息熵的定义为：
	  H（x）=-plog(p);
	意思是一个变量的变化情况可能越多，那么它携带的信息量就越大。信息熵就是对一个状态的函数。
	而信息增益则是针对一个分类系统的某一个特征而言的。就是分析这个系统中有特征T和没有特征T时信息熵减少的差值，即这个特征带给这个系统的信息量多少。信息增益的计算公式如下：
	  ...
其中，S为全部样本的集合，value(T)是属性T所有取值的集合。v是其中一个属性值，  是S中属性T的值为v的样例的集合。

3.	ID3算法
本实验采用ID3(Iterative Dichotomiser 3)算法来构建决策树。根据信息论知识，期望信息越小，信息增益越大，从而纯度越高。
在决策树的每一个非叶子结点划分之前，先计算每一个属性所带来的信息增益，选择最大信息增益的属性来划分，因为信息增益越大，区分样本的能力就越强，越具有代表性，很显然这是一种自顶向下的贪心策略。以上就是ID3算法的核心思想。
